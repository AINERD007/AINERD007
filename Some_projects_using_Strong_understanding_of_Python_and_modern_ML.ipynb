{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOUONadiHugvHxRAQxR8s5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AINERD007/AINERD007/blob/main/Some_projects_using_Strong_understanding_of_Python_and_modern_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are four different types of machine learning projects that leverage a strong understanding of Python and various modern ML tools:\n",
        "\n",
        "Image Recognition with Convolutional Neural Networks (CNNs) using TensorFlow and Keras:\n",
        "\n",
        "Build a deep learning model using TensorFlow and Keras to classify images into different categories (e.g., cats vs. dogs, handwritten digits recognition).\n",
        "Preprocess the image data using data augmentation techniques to increase the size of the training set.\n",
        "Train the CNN model on a large dataset and evaluate its performance on a separate test set."
      ],
      "metadata": {
        "id": "nzUpoR-TnKdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load and preprocess the image dataset\n",
        "train_data = ImageDataGenerator(rescale=1./255).flow_from_directory('train_data/', target_size=(224, 224), batch_size=32)\n",
        "test_data = ImageDataGenerator(rescale=1./255).flow_from_directory('test_data/', target_size=(224, 224), batch_size=32)\n",
        "\n",
        "# Build a CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, epochs=10, validation_data=test_data)\n"
      ],
      "metadata": {
        "id": "1z-zBC4cnTgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Natural Language Processing (NLP) Sentiment Analysis with Scikit-Learn and Pandas:\n",
        "\n",
        "Utilize Scikit-Learn's text processing capabilities to preprocess textual data and extract relevant features.\n",
        "Build a machine learning model, such as logistic regression or support vector machine, to classify text into positive or negative sentiments.\n",
        "Evaluate the model's performance using metrics like accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "KS56G5jFnd8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load and preprocess the text dataset\n",
        "data = pd.read_csv('sentiment_data.csv')\n",
        "X = data['text']\n",
        "y = data['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Build a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iIDIhCQRwl2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Forecasting with Long Short-Term Memory (LSTM) using PyTorch:"
      ],
      "metadata": {
        "id": "00Kiw4lNwyaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Forecasting with Long Short-Term Memory (LSTM) using PyTorch:\n",
        "\n",
        "Implement an LSTM model in PyTorch to predict future values in a time series data, such as stock prices or weather data.\n",
        "Split the time series data into training and test sets and preprocess the data for LSTM input.\n",
        "Train the LSTM model on historical data and evaluate its accuracy in forecasting future values."
      ],
      "metadata": {
        "id": "0JU0cbqSw61v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the time series data\n",
        "data = np.loadtxt('time_series_data.csv', delimiter=',')\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "\n",
        "# Normalize the data\n",
        "mean, std = np.mean(X), np.std(X)\n",
        "X = (X - mean) / std\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Build an LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(1, 64)\n",
        "        self.fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x.unsqueeze(2))\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = LSTMModel()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    y_pred = model(X.unsqueeze(2))\n",
        "    loss = criterion(y_pred.squeeze(), y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model(X.unsqueeze(2))\n",
        "mse = criterion(y_pred.squeeze(), y)\n",
        "print(f\"Mean Squared Error: {mse.item()}\")\n"
      ],
      "metadata": {
        "id": "jyxHuWtgwz6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommender System using Matrix Factorization with NumPy and Pandas"
      ],
      "metadata": {
        "id": "Aoug7dyZw_FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of a simple Recommender System using Matrix Factorization with NumPy and Pandas. For this example, we'll create a small dataset of user-item interactions and use matrix factorization to build a collaborative filtering recommender system."
      ],
      "metadata": {
        "id": "1ou54FP2xSLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample user-item interaction data\n",
        "data = {\n",
        "    'User': ['User1', 'User1', 'User2', 'User2', 'User3', 'User4', 'User4', 'User5'],\n",
        "    'Item': ['Item1', 'Item2', 'Item1', 'Item3', 'Item3', 'Item2', 'Item4', 'Item2'],\n",
        "    'Rating': [5, 4, 3, 5, 2, 4, 1, 3]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create user-item interaction matrix\n",
        "user_item_matrix = df.pivot(index='User', columns='Item', values='Rating').fillna(0)\n",
        "\n",
        "# Convert the user-item matrix to a NumPy array\n",
        "user_item_array = user_item_matrix.values\n",
        "\n",
        "# Define the number of latent features (hidden factors)\n",
        "num_features = 2\n",
        "\n",
        "# Perform matrix factorization using Singular Value Decomposition (SVD)\n",
        "U, sigma, Vt = np.linalg.svd(user_item_array)\n",
        "\n",
        "# Retain only the top 'num_features' singular values and corresponding vectors\n",
        "U = U[:, :num_features]\n",
        "sigma = np.diag(sigma[:num_features])\n",
        "Vt = Vt[:num_features, :]\n",
        "\n",
        "# Reconstruct the user-item matrix using the reduced latent features\n",
        "user_item_reconstructed = np.dot(np.dot(U, sigma), Vt)\n",
        "\n",
        "# Convert the reconstructed matrix back to a DataFrame\n",
        "reconstructed_df = pd.DataFrame(user_item_reconstructed, columns=user_item_matrix.columns, index=user_item_matrix.index)\n",
        "\n",
        "# Recommend items for a specific user\n",
        "def recommend_items(user, num_recommendations=2):\n",
        "    user_row_idx = df['User'].unique().tolist().index(user)\n",
        "    user_ratings = reconstructed_df.iloc[user_row_idx]\n",
        "    top_recommendations = user_ratings.sort_values(ascending=False).index[:num_recommendations]\n",
        "    return top_recommendations\n",
        "\n",
        "# Example usage\n",
        "user_to_recommend = 'User1'\n",
        "recommended_items = recommend_items(user_to_recommend)\n",
        "print(f\"Recommended items for {user_to_recommend}: {recommended_items}\")\n"
      ],
      "metadata": {
        "id": "iQ7VBYWdxN4D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}